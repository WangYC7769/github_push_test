{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 基本处理单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000,    nan, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的数组大小5x4的矩阵\n",
    "a = torch.Tensor(5,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0183, 0.1020, 0.9229, 0.7919],\n",
       "        [0.3963, 0.5906, 0.3186, 0.4597],\n",
       "        [0.1792, 0.8656, 0.6486, 0.2832],\n",
       "        [0.2041, 0.2687, 0.8877, 0.0685],\n",
       "        [0.3728, 0.3687, 0.9240, 0.6478]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的数组大小是5x4的矩阵，初始化是0~1的均匀分布\n",
    "torch.rand(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7286, 0.6617, 0.4844, 0.6407],\n",
      "        [0.4961, 0.0734, 0.6385, 0.3460],\n",
      "        [0.6275, 0.4987, 0.2483, 0.0126],\n",
      "        [0.3542, 0.9990, 0.2551, 0.3168],\n",
      "        [0.7882, 0.1038, 0.2403, 0.3592]])\n",
      "torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# 得到矩阵大小\n",
    "torchtensor = torch.rand(5, 4)\n",
    "print(torchtensor)\n",
    "print(torchtensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 类似的返回5x4大小的矩阵\n",
    "np.ones((5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.93169264 0.3936658 ]\n",
      " [0.59703753 0.48359814]\n",
      " [0.87246651 0.76637981]]\n"
     ]
    }
   ],
   "source": [
    "numpymatrix = np.random.random((3, 2))\n",
    "print(numpymatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0669, 0.9060, 0.3608, 0.9418],\n",
      "        [0.3516, 0.8702, 0.5445, 0.4022],\n",
      "        [0.9874, 0.2100, 0.5974, 0.6802],\n",
      "        [0.8322, 0.8618, 0.9479, 0.7109],\n",
      "        [0.7188, 0.1994, 0.0152, 0.5384]]) \n",
      "\n",
      "[[0.06686485 0.90599525 0.36078322 0.94182914]\n",
      " [0.35164225 0.870247   0.54450774 0.4021746 ]\n",
      " [0.98739105 0.2100156  0.5974472  0.6801666 ]\n",
      " [0.8322172  0.8618167  0.94787705 0.7108633 ]\n",
      " [0.7188453  0.199435   0.015248   0.5383508 ]] \n",
      "\n",
      "torch.Size([5, 4]) \n",
      "\n",
      "(5, 4) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numpy 和 torch.Tensor 之间的转换\n",
    "a = torch.rand(5, 4)\n",
    "b = a.numpy()\n",
    "print(a,'\\n')\n",
    "print(b,'\\n')\n",
    "print(a.size(),'\\n')\n",
    "print(b.shape,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4]\n",
      " [3 6]] \n",
      "\n",
      "tensor([[3, 4],\n",
      "        [3, 6]], dtype=torch.int32) \n",
      "\n",
      "(2, 2) \n",
      "\n",
      "torch.Size([2, 2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[3, 4], [3, 6]])\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "print(a,'\\n')\n",
    "print(b,'\\n')\n",
    "print(a.shape,'\\n')\n",
    "print(b.size(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "a = array([[1, 3], [2, 4], [5, 6]])\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3000],\n",
       "        [ 4.4000],\n",
       "        [ 5.5000],\n",
       "        [ 6.7100],\n",
       "        [ 6.9300],\n",
       "        [ 4.1680],\n",
       "        [ 9.7790],\n",
       "        [ 6.1820],\n",
       "        [ 7.5900],\n",
       "        [ 2.1670],\n",
       "        [ 7.0420],\n",
       "        [10.7910],\n",
       "        [ 5.3130],\n",
       "        [ 7.9970],\n",
       "        [ 3.1000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],\n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042],\n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((3, 5, 2), dtype=np.complex128)\n",
    "#np.prod(x.shape)\n",
    "np.size(x)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运算和numpy类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0664, 0.5140, 0.3413, 0.6868],\n",
       "        [0.4688, 0.5812, 0.4567, 0.5556],\n",
       "        [0.8264, 0.8533, 0.8824, 0.9460],\n",
       "        [0.0607, 0.6102, 0.6844, 0.9470],\n",
       "        [0.7700, 0.8871, 0.6622, 0.2189]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 4)\n",
    "y = torch.rand(5, 4)\n",
    "c = 3\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1992, 1.5420, 1.0238, 2.0605],\n",
      "        [1.4063, 1.7435, 1.3701, 1.6668],\n",
      "        [2.4793, 2.5600, 2.6473, 2.8380],\n",
      "        [0.1821, 1.8307, 2.0531, 2.8409],\n",
      "        [2.3101, 2.6613, 1.9865, 0.6567]])\n",
      "tensor([[0.9534, 0.1911, 0.6159, 0.3474],\n",
      "        [0.9107, 0.4464, 0.5194, 0.7155],\n",
      "        [0.2504, 0.1092, 0.0749, 0.4053],\n",
      "        [0.1373, 0.6465, 0.2087, 0.2540],\n",
      "        [0.3218, 0.9872, 0.5132, 0.4637]])\n"
     ]
    }
   ],
   "source": [
    "print(c * x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0198, 0.7051, 0.9572, 1.0342],\n",
      "        [1.3794, 1.0276, 0.9761, 1.2711],\n",
      "        [1.0769, 0.9625, 0.9573, 1.3513],\n",
      "        [0.1980, 1.2567, 0.8931, 1.2009],\n",
      "        [1.0918, 1.8743, 1.1754, 0.6826]])\n"
     ]
    }
   ],
   "source": [
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0198, 0.7051, 0.9572, 1.0342],\n",
      "        [1.3794, 1.0276, 0.9761, 1.2711],\n",
      "        [1.0769, 0.9625, 0.9573, 1.3513],\n",
      "        [0.1980, 1.2567, 0.8931, 1.2009],\n",
      "        [1.0918, 1.8743, 1.1754, 0.6826]])\n",
      "tensor([[0.0664, 0.5140, 0.3413, 0.6868],\n",
      "        [0.4688, 0.5812, 0.4567, 0.5556],\n",
      "        [0.8264, 0.8533, 0.8824, 0.9460],\n",
      "        [0.0607, 0.6102, 0.6844, 0.9470],\n",
      "        [0.7700, 0.8871, 0.6622, 0.2189]])\n"
     ]
    }
   ],
   "source": [
    "print(x.add(y))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0198, 0.7051, 0.9572, 1.0342],\n",
      "        [1.3794, 1.0276, 0.9761, 1.2711],\n",
      "        [1.0769, 0.9625, 0.9573, 1.3513],\n",
      "        [0.1980, 1.2567, 0.8931, 1.2009],\n",
      "        [1.0918, 1.8743, 1.1754, 0.6826]])\n"
     ]
    }
   ],
   "source": [
    "# 可以直接进行操作改变原对象，x+y或者x.add()并不会改变x，但是x.add_()则会对x进行改变\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0198, 0.7051, 0.9572, 1.0342],\n",
      "        [1.3794, 1.0276, 0.9761, 1.2711],\n",
      "        [1.0769, 0.9625, 0.9573, 1.3513],\n",
      "        [0.1980, 1.2567, 0.8931, 1.2009],\n",
      "        [1.0918, 1.8743, 1.1754, 0.6826]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 torch.Tensor 放到 GPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断一下电脑是否支持GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4459, 0.4001, 0.0289, 0.7369],\n",
      "        [0.5530, 0.2841, 0.6589, 0.4691],\n",
      "        [0.3124, 0.4304, 0.2910, 0.7345],\n",
      "        [0.9492, 0.3983, 0.0062, 0.8460],\n",
      "        [0.7582, 0.4797, 0.6888, 0.6493]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(5, 4)\n",
    "#a = a.cuda()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch 的自动求导功能\n",
    "torch 和大部分框架一样有着自动求导功能，对象不再是 torch.Tensor，而是torch.autograd.Variable\n",
    "\n",
    "本质上Variable和Tensor没有什么区别，不过Variable会放在一个计算图里面，可以进行前向传播和反向传播以及求导  \n",
    "\n",
    "![1.png](http://upload-images.jianshu.io/upload_images/3623720-1c2694b72e0341ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "里面的creator表示通过什么操作得到的这个Variable，grad表示反向传播的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4459, 0.4001, 0.0289, 0.7369],\n",
      "        [0.5530, 0.2841, 0.6589, 0.4691],\n",
      "        [0.3124, 0.4304, 0.2910, 0.7345],\n",
      "        [0.9492, 0.3983, 0.0062, 0.8460],\n",
      "        [0.7582, 0.4797, 0.6888, 0.6493]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = a.cuda()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad 表示是否对其求梯度，默认是False\n",
    "x = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "y = Variable(torch.Tensor([5]), requires_grad=True)\n",
    "z = 2 * x + y**2 + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.], requires_grad=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 x 和 y 分别求导\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor([2.])\n",
      "dz/dy: tensor([10.])\n"
     ]
    }
   ],
   "source": [
    "# x 的导数和 y 的导数\n",
    "print('dz/dx: {}'.format(x.grad.data))\n",
    "print('dz/dy: {}'.format(y.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络部分\n",
    "\n",
    "所依赖的主要是 torch.nn 和 torch.nn.functional\n",
    "\n",
    "torch.nn 里面有着所有的神经网络的层的操作，其用来构建网络，只有执行一次网络的运算才执行一次\n",
    "\n",
    "torch.nn.functional 表示的是直接对其做一次向前运算操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的网络构建类模板\n",
    "class net_name(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_name, self).__init__()\n",
    "        # 可以添加各种网络层\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3)\n",
    "        # 具体每种层的参数可以去查看文档\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 定义向前传播\n",
    "        out = self.conv1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
